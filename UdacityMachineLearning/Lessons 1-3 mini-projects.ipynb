{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "import sys\n",
    "from time import time\n",
    "from os.path import expanduser\n",
    "sys.path.append(expanduser(\"~/Documents/ud120-projects-master/tools/\"))\n",
    "from email_preprocess import preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes as nb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My own exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alabels_test=np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alabels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1758, 3785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates axis 0 is emails, and 1 is words(features?)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1284468 ,  1.77616023,  3.40693072,  3.80264453,  1.        ,\n",
       "        4.18466885,  3.60751176,  3.82152026,  5.67650369,  3.79537177])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test[:10].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7190368141442387"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.sum()/features_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand how this information is coded.  The logical thing to me would be an integer record of whether a word is used in an email, but since it appears that at least most of the first ten contain non integer amounts, that doesn't seem to be the case. Anyway, I can't seem to understand the average length of the emails, which would be an indicator of how confident one can be of writers of the emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1: What is the accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 805 ms, sys: 566 ms, total: 1.37 s\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model=nb.GaussianNB()\n",
    "model.fit(features_train,labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 32.2 ms, total: 149 ms\n",
      "Wall time: 148 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testprediction=model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97326507394766781"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(testprediction==alabels_test).sum()/alabels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is astoundingly good. I would never guess that people are so reliable in their choice of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2: Which takes longer, training or prediction\n",
    "\n",
    "We see that training takes a good deal longer; not surprising at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### My own fooling around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_features_train=features_train[:int(round(features_train.shape[0]/100))]\n",
    "sub_labels_train=labels_train[:int(round(features_train.shape[0]/100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels=['linear', 'poly', 'rbf', 'sigmoid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "CPU times: user 9.98 ms, sys: 12 µs, total: 10 ms\n",
      "Wall time: 9.05 ms\n",
      "0.854948805461\n",
      "poly\n",
      "CPU times: user 11.1 ms, sys: 0 ns, total: 11.1 ms\n",
      "Wall time: 11.1 ms\n",
      "0.522184300341\n",
      "rbf\n",
      "CPU times: user 13.7 ms, sys: 0 ns, total: 13.7 ms\n",
      "Wall time: 13.7 ms\n",
      "0.589874857793\n",
      "sigmoid\n",
      "CPU times: user 10.2 ms, sys: 0 ns, total: 10.2 ms\n",
      "Wall time: 10.2 ms\n",
      "0.492036405006\n"
     ]
    }
   ],
   "source": [
    "for i in kernels:\n",
    "    print(i)\n",
    "    model=svm.SVC(kernel=i)\n",
    "    %time model.fit(sub_features_train,sub_labels_train)\n",
    "    print((model.predict(features_test)==alabels_test).sum()/alabels_test.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. I did not think that the kernel mattered that much. I guess a highly complex kernel requires some tuning of hyper-parameters? I really need to understand better how to tailor svm kernels to the data. It's pretty hard to do though, as long as this data is a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1: What is the accuracy of the linear classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 218 ms, total: 2min 31s\n",
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(features_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 3.88 ms, total: 15.9 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testprediction=model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98407281001137659"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(testprediction==alabels_test)/alabels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2: How do the training and prediction times compare to Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.2173913043478"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training time\n",
    "lsvm=(2*60+13)*1000\n",
    "lsvm/805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135.89743589743588"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction time\n",
    "15.9*1000/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very poorly. Training and prediction times are both well over 100 times longer for SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3: What is the accuracy after shrinking the training set to 1% of it's original size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_features_train=features_train[:int(round(features_train.shape[0]/100))]\n",
    "sub_labels_train=labels_train[:int(round(features_train.shape[0]/100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 92.5 ms, sys: 5 µs, total: 92.5 ms\n",
      "Wall time: 91.3 ms\n",
      "CPU times: user 916 ms, sys: 1e+03 ns, total: 916 ms\n",
      "Wall time: 916 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88452787258248011"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=svm.SVC(kernel='linear')\n",
    "%time model.fit(sub_features_train,sub_labels_train)\n",
    "%time testprediction=model.predict(features_test)\n",
    "np.sum(testprediction==alabels_test)/alabels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an awful lot faster, and doesn't do too bad as far a prediction goes. The prediction time is also drammatically lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4: Which of these are applications where you can imagine a very quick-running algorithm is especially important?\n",
    "\n",
    "Flagging credit card fraud, and blocking a transaction before it goes through and voice recognition, like Siri, both would require quick prediction time. However, training time for both of these can be long. There are very few applications where long training times are not acceptable for the final product, though long training times can definitely make testing difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5: What’s the accuracy with the more complex rbf kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ms, sys: 6 µs, total: 113 ms\n",
      "Wall time: 112 ms\n",
      "CPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\n",
      "Wall time: 1.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61604095563139927"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=svm.SVC(kernel='rbf')\n",
    "%time model.fit(sub_features_train,sub_labels_train)\n",
    "%time testprediction=model.predict(features_test)\n",
    "np.sum(testprediction==alabels_test)/alabels_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67721518987341767"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asub_labels_train=np.array(sub_labels_train)\n",
    "np.sum(model.predict(sub_features_train)==asub_labels_train)/asub_labels_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have guessed that this kernel might just be overfitting the data, but that doesn't quite seem to be the case - it doesn't even predict the training data well. It must be that it is \"underfitting\", and doesn't have the freedom to change the shape to match the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6 & Q7: What value of C gives the best accuracy, and what is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=10\n",
      "fitting:\n",
      "CPU times: user 105 ms, sys: 0 ns, total: 105 ms\n",
      "Wall time: 103 ms\n",
      "prediction:\n",
      "CPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\n",
      "Wall time: 1.06 s\n",
      "accuracy=0.616040955631\n",
      "C=100\n",
      "fitting:\n",
      "CPU times: user 106 ms, sys: 0 ns, total: 106 ms\n",
      "Wall time: 106 ms\n",
      "prediction:\n",
      "CPU times: user 1.15 s, sys: 0 ns, total: 1.15 s\n",
      "Wall time: 1.15 s\n",
      "accuracy=0.616040955631\n",
      "C=1000\n",
      "fitting:\n",
      "CPU times: user 98.5 ms, sys: 0 ns, total: 98.5 ms\n",
      "Wall time: 98.4 ms\n",
      "prediction:\n",
      "CPU times: user 1.04 s, sys: 0 ns, total: 1.04 s\n",
      "Wall time: 1.04 s\n",
      "accuracy=0.821387940842\n",
      "C=10000\n",
      "fitting:\n",
      "CPU times: user 97.2 ms, sys: 0 ns, total: 97.2 ms\n",
      "Wall time: 97.1 ms\n",
      "prediction:\n",
      "CPU times: user 851 ms, sys: 0 ns, total: 851 ms\n",
      "Wall time: 851 ms\n",
      "accuracy=0.892491467577\n",
      "C=100000\n",
      "fitting:\n",
      "CPU times: user 94 ms, sys: 0 ns, total: 94 ms\n",
      "Wall time: 94 ms\n",
      "prediction:\n",
      "CPU times: user 799 ms, sys: 0 ns, total: 799 ms\n",
      "Wall time: 799 ms\n",
      "accuracy=0.860068259386\n",
      "C=1000000\n",
      "fitting:\n",
      "CPU times: user 90.2 ms, sys: 0 ns, total: 90.2 ms\n",
      "Wall time: 90.1 ms\n",
      "prediction:\n",
      "CPU times: user 792 ms, sys: 0 ns, total: 792 ms\n",
      "Wall time: 792 ms\n",
      "accuracy=0.860068259386\n",
      "C=10000000\n",
      "fitting:\n",
      "CPU times: user 93.7 ms, sys: 0 ns, total: 93.7 ms\n",
      "Wall time: 93.8 ms\n",
      "prediction:\n",
      "CPU times: user 780 ms, sys: 24 µs, total: 780 ms\n",
      "Wall time: 783 ms\n",
      "accuracy=0.860068259386\n",
      "C=100000000\n",
      "fitting:\n",
      "CPU times: user 89.4 ms, sys: 1 µs, total: 89.4 ms\n",
      "Wall time: 92.1 ms\n",
      "prediction:\n",
      "CPU times: user 782 ms, sys: 13 µs, total: 782 ms\n",
      "Wall time: 782 ms\n",
      "accuracy=0.860068259386\n",
      "C=1000000000\n",
      "fitting:\n",
      "CPU times: user 98.2 ms, sys: 0 ns, total: 98.2 ms\n",
      "Wall time: 98.2 ms\n",
      "prediction:\n",
      "CPU times: user 797 ms, sys: 0 ns, total: 797 ms\n",
      "Wall time: 797 ms\n",
      "accuracy=0.860068259386\n"
     ]
    }
   ],
   "source": [
    "a=10**np.arange(1,10)\n",
    "for i in a:\n",
    "    model=svm.SVC(kernel='rbf',C=i)\n",
    "    print('C='+str(i))\n",
    "    print('fitting:')\n",
    "    %time model.fit(sub_features_train,sub_labels_train)\n",
    "    print('prediction:')\n",
    "    %time testprediction=model.predict(features_test)\n",
    "    print('accuracy='+str(np.sum(testprediction==alabels_test)/alabels_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be an ideal value for this. The default value, 1, is insufficiantly complex to follow the data and is underfit, and at some point, around C=10000, it shifts to being overfit, as the complexities of the decision boundary allow it to simply follow every single point, with the ideal value around 10000.\n",
    "\n",
    "However, I should note that doing this process is bad data science - we are over-fitting the parameter C to the test data set. Udacity's suggestion doesn't even do this bad process right, it has us stop at 10000, and we don't even know if we could do better by going higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=5000\n",
      "fitting:\n",
      "CPU times: user 103 ms, sys: 25 µs, total: 103 ms\n",
      "Wall time: 102 ms\n",
      "prediction:\n",
      "CPU times: user 851 ms, sys: 0 ns, total: 851 ms\n",
      "Wall time: 851 ms\n",
      "accuracy=0.899317406143\n",
      "C=6111\n",
      "fitting:\n",
      "CPU times: user 93.9 ms, sys: 0 ns, total: 93.9 ms\n",
      "Wall time: 93.9 ms\n",
      "prediction:\n",
      "CPU times: user 859 ms, sys: 0 ns, total: 859 ms\n",
      "Wall time: 859 ms\n",
      "accuracy=0.900455062571\n",
      "C=7222\n",
      "fitting:\n",
      "CPU times: user 95.2 ms, sys: 0 ns, total: 95.2 ms\n",
      "Wall time: 95.2 ms\n",
      "prediction:\n",
      "CPU times: user 877 ms, sys: 0 ns, total: 877 ms\n",
      "Wall time: 877 ms\n",
      "accuracy=0.896473265074\n",
      "C=8333\n",
      "fitting:\n",
      "CPU times: user 93.9 ms, sys: 0 ns, total: 93.9 ms\n",
      "Wall time: 93.9 ms\n",
      "prediction:\n",
      "CPU times: user 899 ms, sys: 0 ns, total: 899 ms\n",
      "Wall time: 899 ms\n",
      "accuracy=0.894197952218\n",
      "C=9444\n",
      "fitting:\n",
      "CPU times: user 93.9 ms, sys: 0 ns, total: 93.9 ms\n",
      "Wall time: 93.9 ms\n",
      "prediction:\n",
      "CPU times: user 858 ms, sys: 0 ns, total: 858 ms\n",
      "Wall time: 858 ms\n",
      "accuracy=0.892491467577\n",
      "C=10555\n",
      "fitting:\n",
      "CPU times: user 96.2 ms, sys: 0 ns, total: 96.2 ms\n",
      "Wall time: 96.2 ms\n",
      "prediction:\n",
      "CPU times: user 866 ms, sys: 0 ns, total: 866 ms\n",
      "Wall time: 866 ms\n",
      "accuracy=0.892491467577\n",
      "C=11666\n",
      "fitting:\n",
      "CPU times: user 94.1 ms, sys: 0 ns, total: 94.1 ms\n",
      "Wall time: 94.1 ms\n",
      "prediction:\n",
      "CPU times: user 842 ms, sys: 0 ns, total: 842 ms\n",
      "Wall time: 842 ms\n",
      "accuracy=0.889078498294\n",
      "C=12777\n",
      "fitting:\n",
      "CPU times: user 94.1 ms, sys: 0 ns, total: 94.1 ms\n",
      "Wall time: 94.1 ms\n",
      "prediction:\n",
      "CPU times: user 864 ms, sys: 0 ns, total: 864 ms\n",
      "Wall time: 865 ms\n",
      "accuracy=0.887940841866\n",
      "C=13888\n",
      "fitting:\n",
      "CPU times: user 94.5 ms, sys: 0 ns, total: 94.5 ms\n",
      "Wall time: 94.6 ms\n",
      "prediction:\n",
      "CPU times: user 842 ms, sys: 0 ns, total: 842 ms\n",
      "Wall time: 842 ms\n",
      "accuracy=0.885096700796\n",
      "C=15000\n",
      "fitting:\n",
      "CPU times: user 103 ms, sys: 0 ns, total: 103 ms\n",
      "Wall time: 103 ms\n",
      "prediction:\n",
      "CPU times: user 908 ms, sys: 0 ns, total: 908 ms\n",
      "Wall time: 908 ms\n",
      "accuracy=0.883390216155\n"
     ]
    }
   ],
   "source": [
    "a=np.linspace(10000-5000,10000+5000,num=10,dtype=int)\n",
    "for i in a:\n",
    "    model=svm.SVC(kernel='rbf',C=i)\n",
    "    print('C='+str(i))\n",
    "    print('fitting:')\n",
    "    %time model.fit(sub_features_train,sub_labels_train)\n",
    "    print('prediction:')\n",
    "    %time testprediction=model.predict(features_test)\n",
    "    print('accuracy='+str(np.sum(testprediction==alabels_test)/alabels_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q8: What is the accuracy with the full set and \"optimized\" C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=svm.SVC(kernel='rbf',C=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 88 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(features_train,labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a while, but it was still an awful lot shorter than the 15 minutes it took with C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 20 µs, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testprediction=model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_test=np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99089874857792948"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(testprediction==labels_test))/labels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's so good that I have a hard time believing it. As I was using the test set to choose my C, it's probable that I have overfitted to the test set.  In order to do this process properly, the training set should be devided into subsets, and one subset used for training, one for testing, and then the acuraccy should be maximized with respect to the parameter. Then you can test on a larger test and training set to get a real understanding of the accuracy. The sklearn.grid_search.GridSearchCV method uses cross validation, a slightly more complex version of what I just described to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q9: What is the prediction for these examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elm 10 prediction = 1, actual = 1\n",
      "elm 26 prediction = 0, actual = 0\n",
      "elm 50 prediction = 1, actual = 1\n"
     ]
    }
   ],
   "source": [
    "print('elm 10 prediction = ' + str(testprediction[10])+', actual = '+ str(labels_test[10]))\n",
    "print('elm 26 prediction = ' + str(testprediction[26])+', actual = '+ str(labels_test[26]))\n",
    "print('elm 50 prediction = ' + str(testprediction[50])+', actual = '+ str(labels_test[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(testprediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1: What is the accuracy with the minimum sample split equal to 40?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(min_samples_split=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.8 s, sys: 76.1 ms, total: 57.8 s\n",
      "Wall time: 57.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=40, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(features_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 8.01 ms, total: 22.2 ms\n",
      "Wall time: 21 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testprediction=model.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_test=np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97724687144482369"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(testprediction==labels_test).sum()/labels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2: Speeding Up Via Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3785"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature selection algorithm is selecting only the features that are most well correlated with the data, with the correlation in this case measured by a $\\chi ^2$ correlation test between the feature and the labels. In this case, we're picking out the top 10% most highly correlated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3: Smaller number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "from email_preprocess import preprocesssmall\n",
    "features_train, features_test, labels_train, labels_test = preprocesssmall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a smaller number of variables, we cannot have a less complex decision surface. If we add a completely random feature, however, with an ideal machine learning algorithm there should be no increased complexity, and in general a good algorithm should only increase the complexity of the decision surface with more features if the new features add useful information about the labels.\n",
    "\n",
    "Since we are dropping features that are fairly highly correlated with the labels, this will decrease the complexity of the decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5: Accuracy with less features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.78 s, sys: 14 µs, total: 3.78 s\n",
      "Wall time: 3.78 s\n",
      "CPU times: user 1.9 ms, sys: 0 ns, total: 1.9 ms\n",
      "Wall time: 1.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96643913538111492"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time model.fit(features_train,labels_train)\n",
    "%time testprediction=model.predict(features_test)\n",
    "labels_test=np.array(labels_test)\n",
    "(testprediction==labels_test).sum()/labels_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these two data points (hardly enough data to really figure out), I'd guess that model fitting and prediction time scales at least linearly with the number of features, if not with some higher power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
